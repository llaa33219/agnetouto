# AgentOutO

**ë©€í‹° ì—ì´ì „íŠ¸ íŠ¹í™” Python SDK â€” ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ì—†ëŠ” í”¼ì–´ ê°„ ìžìœ  í˜¸ì¶œ**

A multi-agent Python SDK where every agent is equal. No orchestrator. No hierarchy. No restrictions.

---

## í•µì‹¬ ì² í•™ (Core Philosophy)

AgentOutO rejects the orchestrator pattern used by existing frameworks (CrewAI, AutoGen, etc.).

> **ëª¨ë“  ì—ì´ì „íŠ¸ëŠ” ì™„ì „ížˆ ëŒ€ë“±í•˜ë‹¤.** Base ì—ì´ì „íŠ¸ê°€ ì¡´ìž¬í•˜ì§€ ì•ŠëŠ”ë‹¤.
>
> **ëª¨ë“  ì—ì´ì „íŠ¸ëŠ” ëª¨ë“  ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìžˆë‹¤.** í˜¸ì¶œ ì œí•œì´ ì—†ë‹¤.
>
> **ëª¨ë“  ì—ì´ì „íŠ¸ëŠ” ëª¨ë“  ë„êµ¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìžˆë‹¤.** ë„êµ¬ ì œí•œì´ ì—†ë‹¤.
>
> **ë©”ì‹œì§€ í”„ë¡œí† ì½œì€ ì „ë‹¬/ë°˜í™˜ 2ì¢…ë¥˜ë¿ì´ë‹¤.**
>
> **ì‚¬ìš©ìžëŠ” LLMì´ ì—†ëŠ” ì—ì´ì „íŠ¸ì¼ ë¿ì´ë‹¤.** ë³„ë„ì˜ ì¸í„°íŽ˜ì´ìŠ¤, í”„ë¡œí† ì½œ, ë„êµ¬ëŠ” ì¡´ìž¬í•˜ì§€ ì•ŠëŠ”ë‹¤.

| Existing Frameworks | AgentOutO |
|---|---|
| Orchestrator-centric hierarchy | Peer-to-peer free calls |
| Base agent required | No base agent |
| Per-agent allowed-call lists | Any agent calls any agent |
| Per-agent tool assignment | All tools are global |
| Complex message protocols | Forward / Return only |
| Top-down message flow | Bidirectional free flow |

---

## Installation

```bash
pip install agnetouto
```

Requires Python â‰¥ 3.11.

---

## Quick Start

```python
from agnetouto import Agent, Tool, Provider, run

# Provider â€” API connection info only
openai = Provider(name="openai", kind="openai", api_key="sk-...")

# Tool â€” globally available to all agents
@Tool
def search_web(query: str) -> str:
    """Search the web."""
    return f"Results for: {query}"

# Agent â€” model settings live here
researcher = Agent(
    name="researcher",
    instructions="Research expert. Search and organize information.",
    model="gpt-4o",
    provider="openai",
)

writer = Agent(
    name="writer",
    instructions="Skilled writer. Turn research into polished reports.",
    model="gpt-4o",
    provider="openai",
)

# Run â€” user is just an agent without an LLM
result = run(
    entry=researcher,
    message="Write an AI trends report.",
    agents=[researcher, writer],
    tools=[search_web],
    providers=[openai],
)

print(result.output)
```

---

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        run()                            â”‚
â”‚              (User = LLM-less agent)                    â”‚
â”‚                         â”‚                               â”‚
â”‚                    Forward Message                      â”‚
â”‚                         â–¼                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Agent Loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚                                             â”‚        â”‚
â”‚  â”‚  â”Œâ”€â”€â†’ LLM Call (via Provider Backend)       â”‚        â”‚
â”‚  â”‚  â”‚        â”‚                                 â”‚        â”‚
â”‚  â”‚  â”‚        â”œâ”€â”€ tool_call  â†’ Tool.execute()   â”‚        â”‚
â”‚  â”‚  â”‚        â”‚                   â”‚             â”‚        â”‚
â”‚  â”‚  â”‚        â”‚              result back â”€â”€â”€â”   â”‚        â”‚
â”‚  â”‚  â”‚        â”‚                             â”‚   â”‚        â”‚
â”‚  â”‚  â”‚        â”œâ”€â”€ call_agent â†’ New Loop â”€â”€â”€â”€â”¤   â”‚        â”‚
â”‚  â”‚  â”‚        â”‚                  â”‚          â”‚   â”‚        â”‚
â”‚  â”‚  â”‚        â”‚             return back â”€â”€â”€â”â”‚   â”‚        â”‚
â”‚  â”‚  â”‚        â”‚                            â”‚â”‚   â”‚        â”‚
â”‚  â”‚  â”‚        â””â”€â”€ finish â†’ Return Message  â”‚â”‚   â”‚        â”‚
â”‚  â”‚  â”‚                                     â”‚â”‚   â”‚        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ next iteration â—„â”€â”€â”€â”€â”€â”€â”˜â”˜   â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                         â”‚                               â”‚
â”‚                    Return Message                       â”‚
â”‚                         â–¼                               â”‚
â”‚                    RunResult.output                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Message Flow â€” Peer to Peer

```
[User]  â”€â”€(forward)â”€â”€â†’  [Agent A]
                            â”‚
                            â”œâ”€â”€(forward)â”€â”€â†’ [Agent B]
                            â”‚                 â”œâ”€â”€(forward)â”€â”€â†’ [Agent C]
                            â”‚                 â”‚                  â”‚
                            â”‚                 â”‚â†â”€â”€(return)â”€â”€â”€â”€â”€â”€â”˜
                            â”‚                 â”‚
                            â”‚â†â”€â”€(return)â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â””â”€â”€(return)â”€â”€â†’  [User]
```

Userâ†’A and Aâ†’B use the **exact same mechanism**. There is no special user protocol.

### Parallel Calls

```
[Agent A]
    â”œâ”€â”€(forward)â”€â”€â†’ [Agent B]  â”€â”
    â”œâ”€â”€(forward)â”€â”€â†’ [Agent C]   â”œâ”€â”€ asyncio.gather â€” all run concurrently
    â””â”€â”€(forward)â”€â”€â†’ [Agent D]  â”€â”˜
                                â”‚
    â†â”€â”€(3 returns, batched)â”€â”€â”€â”€â”˜
```

---

## Core Concepts

### Provider â€” API Connection Only

Providers hold API credentials. No model settings, no inference config.

```python
from agnetouto import Provider

openai = Provider(name="openai", kind="openai", api_key="sk-...")
anthropic = Provider(name="anthropic", kind="anthropic", api_key="sk-ant-...")
google = Provider(name="google", kind="google", api_key="AIza...")

# OpenAI-compatible APIs (vLLM, Ollama, LM Studio, etc.)
local = Provider(name="local", kind="openai", base_url="http://localhost:11434/v1")
```

| Field | Description | Required |
|-------|-------------|----------|
| `name` | Identifier for the provider | âœ… |
| `kind` | API type: `"openai"`, `"anthropic"`, `"google"` | âœ… |
| `api_key` | API key | âœ… |
| `base_url` | Custom endpoint URL (for compatible APIs) | âŒ |

### Agent â€” Model Settings Live Here

```python
from agnetouto import Agent

agent = Agent(
    name="researcher",
    instructions="Research expert.",
    model="gpt-4o",
    provider="openai",
    max_output_tokens=16384,
    reasoning=True,
    reasoning_effort="high",
    temperature=1.0,
)
```

| Field | Description | Default |
|-------|-------------|---------|
| `name` | Agent name | (required) |
| `instructions` | Role description | (required) |
| `model` | Model name | (required) |
| `provider` | Provider name | (required) |
| `max_output_tokens` | Max output tokens | `4096` |
| `reasoning` | Enable reasoning/thinking mode | `False` |
| `reasoning_effort` | Reasoning intensity | `"medium"` |
| `reasoning_budget` | Thinking token budget (Anthropic) | `None` |
| `temperature` | Temperature | `1.0` |
| `extra` | Additional API parameters (free dict) | `{}` |

The SDK uses unified parameter names. Each provider backend maps them internally:

| SDK Parameter | OpenAI | Anthropic | Google Gemini |
|---|---|---|---|
| `max_output_tokens` | `max_completion_tokens` | `max_tokens` | `max_output_tokens` (in generation_config) |
| `reasoning=True` | sends `reasoning_effort` | `thinking={"type": "enabled", "budget_tokens": ...}` | `thinking_config={"thinking_budget": ...}` |
| `reasoning_effort` | top-level `reasoning_effort` | N/A | N/A |
| `reasoning_budget` | N/A | `thinking.budget_tokens` | `thinking_config.thinking_budget` |
| `temperature` (reasoning=True) | **not sent** | **forced to 1** | sent as-is |

See [`ai-docs/PROVIDER_BACKENDS.md`](./ai-docs/PROVIDER_BACKENDS.md) for full mapping details.

### Tool â€” Global, No Per-Agent Restrictions

```python
from agnetouto import Tool

@Tool
def search_web(query: str) -> str:
    """Search the web."""
    return f"Results for: {query}"

# Async tools are supported
@Tool
async def fetch_data(url: str) -> str:
    """Fetch data from URL."""
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as resp:
            return await resp.text()
```

Tools are automatically converted to JSON schemas from function signatures and docstrings. All agents can use all tools.

### Message â€” Forward and Return Only

```python
@dataclass
class Message:
    type: Literal["forward", "return"]
    sender: str
    receiver: str
    content: str
    call_id: str  # Unique tracking ID
```

Two types. No exceptions.

---

## Supported Providers

| Kind | Provider | Compatible With |
|------|----------|-----------------|
| `"openai"` | OpenAI API | vLLM, Ollama, LM Studio, any OpenAI-compatible API |
| `"anthropic"` | Anthropic API | â€” |
| `"google"` | Google Gemini API | â€” |

---

## Async Usage

```python
import asyncio
from agnetouto import async_run

result = await async_run(
    entry=researcher,
    message="Write an AI trends report.",
    agents=[researcher, writer, reviewer],
    tools=[search_web, write_file],
    providers=[openai, anthropic, google],
)
```

---

## Package Structure

```
agnetouto/
â”œâ”€â”€ __init__.py          # Public API: Agent, Tool, Provider, run, async_run, Message, RunResult
â”œâ”€â”€ agent.py             # Agent dataclass
â”œâ”€â”€ tool.py              # Tool decorator/class with auto JSON schema generation
â”œâ”€â”€ message.py           # Message dataclass (forward/return)
â”œâ”€â”€ provider.py          # Provider dataclass (API connection info)
â”œâ”€â”€ context.py           # Per-agent conversation context management
â”œâ”€â”€ router.py            # Message routing, system prompt generation, tool schema building
â”œâ”€â”€ runtime.py           # Agent loop engine, parallel execution, run()/async_run()
â”œâ”€â”€ _constants.py        # Shared constants (CALL_AGENT, FINISH)
â”œâ”€â”€ exceptions.py        # ProviderError, AgentError, ToolError, RoutingError
â””â”€â”€ providers/
    â”œâ”€â”€ __init__.py      # ProviderBackend ABC, LLMResponse, get_backend()
    â”œâ”€â”€ openai.py        # OpenAI (+ compatible APIs) implementation
    â”œâ”€â”€ anthropic.py     # Anthropic implementation
    â””â”€â”€ google.py        # Google Gemini implementation
```

---

## Development Status

| Phase | Description | Status |
|-------|-------------|--------|
| **1** | Core classes: Provider, Agent, Tool, Message | âœ… Done |
| **2** | Single agent execution: agent loop + tool calling | âœ… Done |
| **3** | Multi-agent: call_agent + finish + message routing | âœ… Done |
| **4** | Parallel calls: asyncio.gather concurrent execution | âœ… Done |
| **5** | Streaming, logging, debug tools | ðŸ”² Not started |
| **6** | PyPI publish + documentation | ðŸ”² Not started |

---

## Technical Documentation

For AI contributors and detailed technical reference, see **[`ai-docs/`](./ai-docs/)**:

- [`AI_INSTRUCTIONS.md`](./ai-docs/AI_INSTRUCTIONS.md) â€” **Read this first.** How to work on this project and update docs.
- [`PHILOSOPHY.md`](./ai-docs/PHILOSOPHY.md) â€” Core philosophy and inviolable principles.
- [`ARCHITECTURE.md`](./ai-docs/ARCHITECTURE.md) â€” Package structure, module responsibilities, data flow.
- [`PROVIDER_BACKENDS.md`](./ai-docs/PROVIDER_BACKENDS.md) â€” Provider system, parameter mapping, API-specific behavior.
- [`MESSAGE_PROTOCOL.md`](./ai-docs/MESSAGE_PROTOCOL.md) â€” Message types, routing rules, parallel calls, agent loop.
- [`CONVENTIONS.md`](./ai-docs/CONVENTIONS.md) â€” Coding conventions, patterns, naming, style guide.
- [`ROADMAP.md`](./ai-docs/ROADMAP.md) â€” Current status, planned features, known issues.

---

## License

Apache License 2.0 â€” see [LICENSE](./LICENSE) for details.
